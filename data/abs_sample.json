{
    "http://arxiv.org/abs/1512.08849v2": {
        "informative_line_0: ": {
            "Txt": "On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art.",
            "results: ": [
                [
                    "accuracy"
                ],
                [
                    "86.1%"
                ],
                [
                    [
                        "accuracy",
                        "86.1%",
                        "SNLI"
                    ]
                ]
            ],
            "recommend metric": {
                "IOU accuracy": 86,
                "Accuracy": 60,
                "Pearson Correlation": 42,
                "Word Error Rate": 42
            },
            "recommend dataset": {
                "Labeled Faces in the Wild": 86,
                "New York Times Corpus": 86,
                "SNLI": 60,
                "SQuAD1.1": 57
            },
            "Need Skip?": "No"
        }
    },
    "http://arxiv.org/abs/1709.04348v2": {
        "informative_line_0: ": {
            "Txt": "It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.",
            "results: ": [
                [
                    "error"
                ],
                [
                    "20%"
                ],
                [
                    [
                        "error",
                        "20%",
                        "MultiNLI"
                    ]
                ]
            ],
            "recommend metric": {
                "Diarization Error Rate": 86,
                "EM": 60,
                "Error": 60,
                "Error Rate": 57
            },
            "recommend dataset": {
                "Labeled Faces in the Wild": 86,
                "MPII Multi-Person": 57,
                "Children's Book Test": 57,
                "IJB-A": 57
            },
            "Need Skip?": "Yes"
        }
    },
    "http://arxiv.org/abs/1708.01353v1": {
        "informative_line_0: ": {
            "Txt": "We obtain an accuracy of 85.5%, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017.",
            "results: ": [
                [
                    "accuracy"
                ],
                [
                    "85.5%"
                ],
                [
                    [
                        "accuracy",
                        "85.5%",
                        "SNLI"
                    ]
                ]
            ],
            "recommend metric": {
                "Accuracy": 60,
                "F0.5": 57,
                "IOU accuracy": 57,
                "Diarization Error Rate": 50
            },
            "recommend dataset": {
                "Labeled Faces in the Wild": 86,
                "SNLI": 60,
                "SST-5": 57,
                "bAbi": 45
            },
            "Need Skip?": "No"
        }
    },
    "http://arxiv.org/abs/1609.06038v3": {
        "informative_line_0: ": {
            "Txt": "In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset",
            "results: ": [
                [
                    "accuracy"
                ],
                [
                    "88.6%"
                ],
                [
                    [
                        "accuracy",
                        "88.6%"
                    ]
                ]
            ],
            "recommend metric": {
                "Accuracy": 60,
                "AP": 60,
                "IOU accuracy": 57,
                "RMSE": 45
            },
            "recommend dataset": {
                "Labeled Faces in the Wild": 86,
                "New York Times Corpus": 86,
                "TREC-6": 57,
                "Stanford Cars": 57
            },
            "Need Skip?": "No"
        }
    },
    "http://arxiv.org/abs/1809.10853v3": {
        "informative_line_0: ": {
            "Txt": "On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.",
            "results: ": [
                [
                    "perplexity",
                    "perplexity",
                    "perplexity"
                ],
                [
                    "18.7",
                    "10.5",
                    "23.02"
                ],
                [
                    [
                        "perplexity",
                        "18.7"
                    ],
                    [
                        "perplexity",
                        "10.5"
                    ],
                    [
                        "perplexity",
                        "23.02"
                    ]
                ]
            ],
            "recommend metric": {
                "IOU": 60,
                "perplexity": 60,
                "F0.5": 57,
                "Word Error Rate": 57
            },
            "recommend dataset": {
                "WikiText-103": 60,
                "Billion Word": 60,
                "WikiText-2": 57,
                "STL-10": 57
            },
            "Need Skip?": "No"
        }
    },
    "https://arxiv.org/abs/1703.07220v3": {
        "informative_line_0: ": {
            "Txt": "We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92% on Market-1501",
            "results: ": [
                [
                    "accuracy"
                ],
                [
                    "2.92%"
                ],
                [
                    [
                        "accuracy",
                        "2.92%",
                        "Market-1501"
                    ]
                ]
            ],
            "recommend metric": {
                "Accuracy": 60,
                "AP": 60,
                "IOU accuracy": 57,
                "Temporal awareness": 45
            },
            "recommend dataset": {
                "SST-2 Binary classification": 86,
                "GTAV-to-Cityscapes": 86,
                "Labeled Faces in the Wild": 86,
                "New York Times Corpus": 86
            }
        }
    },
    "http://arxiv.org/abs/1711.10485v1": {
        "informative_line_0: ": {
            "Txt": "The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset",
            "results: ": [
                [
                    "inception score"
                ],
                [
                    "14.14%",
                    "170.25%"
                ],
                [
                    [
                        "inception score",
                        "14.14%",
                        "CUB"
                    ]
                ]
            ],
            "recommend metric": {
                "Frechet Inception Distance": 86,
                "Inception Score": 60,
                "BLEU Score": 57,
                "Dice Score": 57
            },
            "recommend dataset": {
                "Labeled Faces in the Wild": 86,
                "COCO": 57,
                "CUB": 57,
                "Set5": 45
            },
            "Need Skip?": "Yes"
        }
    },
    "http://arxiv.org/abs/1708.02182v1": {
        "informative_line_0: ": {
            "Txt": "In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",
            "results: ": [
                [
                    "perplexity"
                ],
                [
                    "52.8",
                    "52.0"
                ],
                [
                    [
                        "perplexity",
                        "52.8",
                        "Penn Treebank"
                    ]
                ]
            ],
            "recommend metric": {
                "perplexity": 60,
                "WER": 60,
                "Frechet Inception Distance": 49,
                "DER": 40
            },
            "recommend dataset": {
                "SST-2 Binary classification": 86,
                "Labeled Faces in the Wild": 86,
                "WikiText-2": 60,
                "Penn Treebank": 60
            },
            "Need Skip?": "No"
        }
    },
    "https://arxiv.org/abs/1810.04805v2": {
        "informative_line_0: ": {
            "Txt": "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "results: ": [
                [
                    "GLUE",
                    "accuracy",
                    "F1",
                    "F1"
                ],
                [
                    "80.5%",
                    "7.7%",
                    "86.7%",
                    "4.6%",
                    "1.1",
                    "93.2",
                    "1.5",
                    "2.0",
                    "83.1",
                    "5.1"
                ],
                [
                    [
                        "GLUE",
                        "80.5%"
                    ],
                    [
                        "accuracy",
                        "86.7%"
                    ],
                    [
                        "F1",
                        "93.2"
                    ],
                    [
                        "F1",
                        "83.1"
                    ]
                ]
            ],
            "recommend metric": {
                "Accuracy": 60,
                "GLUE Score": 60,
                "Score": 60,
                "BLEU Score": 57
            },
            "recommend dataset": {
                "squad": 60,
                "MultiNLI": 60,
                "WikiText-2": 57,
                "SST-5": 57
            },
            "Need Skip?": "No"
        }
    }
}