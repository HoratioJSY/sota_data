{
    "http://arxiv.org/abs/1709.04696v3": {
        "informative_line: ": [
            "It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02% on the Stanford Natural Language Inference (SNLI) dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets."
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "1.02%"
            ]
        ]
    },
    "http://arxiv.org/abs/1709.04348v2": {
        "informative_line: ": [
            "It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system."
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "20%"
            ]
        ]
    },
    "http://arxiv.org/abs/1708.01353v1": {
        "informative_line: ": [
            "We obtain an accuracy of 85.5%, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017."
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "85.5%"
            ]
        ]
    },
    "http://arxiv.org/abs/1609.06038v3": {
        "informative_line: ": [
            "In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "88.6%"
            ]
        ]
    },
    "http://arxiv.org/abs/1809.10853v3": {
        "informative_line: ": [
            "On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity."
        ],
        "results: ": [
            [
                "perplexity",
                "perplexity",
                "perplexity"
            ],
            [
                "18.7",
                "10.5",
                "23.02"
            ]
        ]
    },
    "https://arxiv.org/abs/1703.07220v3": {
        "informative_line: ": [
            "We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92% on Market-1501"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "2.92%"
            ]
        ]
    },
    "http://arxiv.org/abs/1606.02891v2": {
        "informative_line: ": [
            "All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems"
        ],
        "results: ": [
            [
                "BLEU"
            ],
            [
                "4.3",
                "11.2"
            ]
        ]
    },
    "http://arxiv.org/abs/1711.10485v1": {
        "informative_line: ": [
            "The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset"
        ],
        "results: ": [
            [
                "score"
            ],
            [
                "14.14%",
                "170.25%"
            ]
        ]
    },
    "http://arxiv.org/abs/1303.5778v1": {
        "informative_line: ": [
            "When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
        ],
        "results: ": [
            [
                "error",
                "score"
            ],
            [
                "17.7%"
            ]
        ]
    },
    "http://arxiv.org/abs/1806.03489v1": {
        "informative_line: ": [
            "We establish a new state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with a F1 score of 91.73 on the over-studied CONLL-2003 dataset."
        ],
        "results: ": [
            [
                "F1",
                "F1"
            ],
            [
                "87.95",
                "5.0",
                "91.73"
            ]
        ]
    },
    "http://arxiv.org/abs/1506.07503v1": {
        "informative_line: ": [
            "We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on"
        ],
        "results: ": [
            [
                "error rate"
            ],
            [
                "18.7%"
            ]
        ]
    },
    "http://arxiv.org/abs/1710.06481v2": {
        "informative_line: ": [
            "While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% - leaving ample room for improvement."
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "42.9%",
                "74.0%"
            ]
        ]
    },
    "http://arxiv.org/abs/1809.11096v2": {
        "informative_line: ": [
            "When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6."
        ],
        "results: ": [
            [
                "Score"
            ],
            [
                "166.5",
                "7.4",
                "52.52",
                "18.6"
            ]
        ]
    },
    "http://arxiv.org/abs/1803.06589v2": {
        "informative_line: ": [
            "The decision tree classifier satisfies both accuracy and interpretability better than the other classifiers, producing an F1-score and AUC equal to 0.91 and 0.93, respectively"
        ],
        "results: ": [
            [
                "accuracy",
                "F1-score",
                "AUC"
            ],
            [
                "0.91",
                "0.93"
            ]
        ]
    },
    "http://arxiv.org/abs/1508.07909v5": {
        "informative_line: ": [
            "We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively."
        ],
        "results: ": [
            [
                "BLEU"
            ],
            [
                "1.1",
                "1.3"
            ]
        ]
    },
    "http://arxiv.org/abs/1904.07850v2": {
        "informative_line: ": [
            "CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS"
        ],
        "results: ": [
            [
                "AP",
                "AP",
                "AP"
            ],
            [
                "28.1%",
                "37.4%",
                "45.1%",
                "1.4"
            ]
        ]
    },
    "http://arxiv.org/abs/1904.07850v1": {
        "informative_line: ": [
            "CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS"
        ],
        "results: ": [
            [
                "AP",
                "AP",
                "AP"
            ],
            [
                "28.1%",
                "37.4%",
                "45.1%",
                "1.4"
            ]
        ]
    },
    "http://arxiv.org/abs/1904.08189v3": {
        "informative_line: ": [
            "On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%"
        ],
        "results: ": [
            [
                "AP"
            ],
            [
                "47.0%",
                "4.9%"
            ]
        ]
    },
    "https://arxiv.org/abs/1901.00603v2": {
        "informative_line: ": [
            "On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders."
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "70.6%",
                "3%"
            ]
        ]
    },
    "http://arxiv.org/abs/1806.02750v1": {
        "informative_line: ": [
            "The proposed method is validated on the JIGSAWS dataset and achieved very competitive results with 100% accuracy on the suturing and needle passing tasks"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "100%"
            ]
        ]
    },
    "http://arxiv.org/abs/1412.5567v2": {
        "informative_line: ": [
            "Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set"
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "16.0%"
            ]
        ]
    },
    "http://arxiv.org/abs/1610.05256v2": {
        "informative_line: ": [
            "The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations",
            "In both cases, our automated system establishes a new state of the art, and edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively"
        ],
        "results: ": [
            [
                "error rate"
            ],
            [
                "5.9%",
                "11.3%"
            ]
        ]
    },
    "http://arxiv.org/abs/1511.06241v2": {
        "informative_line: ": [
            "Specifically, we obtained a test accuracy of 74.1% on STL-10 and a test error of 0.5% on MNIST."
        ],
        "results: ": [
            [
                "accuracy",
                "error"
            ],
            [
                "74.1%",
                "0.5%"
            ]
        ]
    },
    "http://arxiv.org/abs/1711.04956v5": {
        "informative_line: ": [
            "On the larger WMT'14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art."
        ],
        "results: ": [
            [
                "BLEU"
            ],
            [
                "41.5"
            ]
        ]
    },
    "http://arxiv.org/abs/1802.08948v2": {
        "informative_line: ": [
            "Based on VGG16, it achieves an F-measure of 84.3% on ICDAR2015 and 81.5% on MSRA-TD500."
        ],
        "results: ": [
            [
                "F-measure"
            ],
            [
                "84.3%",
                "81.5%"
            ]
        ]
    },
    "http://arxiv.org/abs/1808.01244v2": {
        "informative_line: ": [
            "Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors."
        ],
        "results: ": [
            [
                "AP"
            ],
            [
                "42.2%"
            ]
        ]
    },
    "http://arxiv.org/abs/1904.08900v1": {
        "informative_line: ": [
            "CornerNet-Saccade is suitable for offline processing, improving the efficiency of CornerNet by 6.0x and the AP by 1.0% on COCO",
            "CornerNet-Squeeze is suitable for real-time detection, improving both the efficiency and accuracy of the popular real-time detector YOLOv3 (34.4% AP at 34ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for YOLOv3 on COCO)"
        ],
        "results: ": [
            [
                "AP"
            ],
            [
                "6.0",
                "1.0%"
            ]
        ]
    },
    "http://arxiv.org/abs/1708.02863v1": {
        "informative_line: ": [
            "a mAP of 82.7% on VOC07, 80.4% on VOC12, and 34.4% on COCO"
        ],
        "results: ": [
            [
                "mAP"
            ],
            [
                "82.7%",
                "80.4%",
                "34.4%"
            ]
        ]
    },
    "http://arxiv.org/abs/1805.04855v1": {
        "informative_line: ": [
            "By doing so, we are able to achieve a recognition accuracy of 58.14% on the validation set of Static Facial Expressions in the Wild (SFEW 2.0) and 87.0% on the vali- dation set of Real-World Affective Faces (RAF) Database"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "58.14%",
                "2.0",
                "87.0%"
            ]
        ]
    },
    "http://arxiv.org/abs/1712.07101v1": {
        "informative_line: ": [
            "The model achieves 5.53% WER on Wall Street Journal dataset, and 5.42% and 14.70% on Librispeech test-clean and test-other set, respectively."
        ],
        "results: ": [
            [
                "WER"
            ],
            [
                "5.53%",
                "5.42%",
                "14.70%"
            ]
        ]
    },
    "http://arxiv.org/abs/1708.04552v2": {
        "informative_line: ": [
            "We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56%, 15.20%, and 1.30% test error respectively"
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "2.56%",
                "15.20%",
                "1.30%"
            ]
        ]
    },
    "http://arxiv.org/abs/1312.6082v4": {
        "informative_line: ": [
            "We report a $99.8\\%$ accuracy on the hardest category of reCAPTCHA"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "99.8"
            ]
        ]
    },
    "http://arxiv.org/abs/1710.02286v1": {
        "informative_line: ": [
            "Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100, compared to the previous state-of-the-art result of 65.43 %"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "67.68",
                "65.43"
            ]
        ]
    },
    "http://arxiv.org/abs/1509.08967v2": {
        "informative_line: ": [
            "We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages",
            "We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far."
        ],
        "results: ": [
            [
                "WER"
            ],
            [
                "5.77%"
            ]
        ]
    },
    "http://arxiv.org/abs/1606.04199v3": {
        "informative_line: ": [
            "On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points",
            "This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points",
            "After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4"
        ],
        "results: ": [
            [
                "BLEU"
            ],
            [
                "37.7",
                "6.2"
            ]
        ]
    },
    "http://arxiv.org/abs/1412.7062v4": {
        "informative_line: ": [
            "Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "71.6%"
            ]
        ]
    },
    "http://arxiv.org/abs/1509.05371v2": {
        "informative_line: ": [
            "On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%",
            "For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "99.2%"
            ]
        ]
    },
    "http://arxiv.org/abs/1412.6575v4": {
        "informative_line: ": [
            "We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "73.2%"
            ]
        ]
    },
    "http://arxiv.org/abs/1808.07042v2": {
        "informative_line: ": [
            "The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement"
        ],
        "results: ": [
            [
                "F1"
            ],
            [
                "65.4%",
                "23.4",
                "88.8%"
            ]
        ]
    },
    "http://arxiv.org/abs/1808.02194v2": {
        "informative_line: ": [
            "The results show that our approach achieves state-of-the-art localization accuracy, but using ~70% fewer parameters, ~98% less model size and saving ~75% training memory compared with other benchmark localizers"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "70%",
                "98%",
                "75%"
            ]
        ]
    },
    "http://arxiv.org/abs/1809.02983v4": {
        "informative_line: ": [
            "In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data"
        ],
        "results: ": [
            [
                "score"
            ],
            [
                "81.5%"
            ]
        ]
    },
    "http://arxiv.org/abs/1802.03268v2": {
        "informative_line: ": [
            "On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing",
            "On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%."
        ],
        "results: ": [
            [
                "perplexity"
            ],
            [
                "55.8"
            ]
        ]
    },
    "http://arxiv.org/abs/1803.08904v1": {
        "informative_line: ": [
            "Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017",
            "Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers"
        ],
        "results: ": [
            [
                "score"
            ],
            [
                "0.5567"
            ]
        ]
    },
    "http://arxiv.org/abs/1903.11816v1": {
        "informative_line: ": [
            "By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster."
        ],
        "results: ": [
            [
                "score"
            ],
            [
                "53.13%",
                "0.5584"
            ]
        ]
    },
    "http://arxiv.org/abs/1709.07634v2": {
        "informative_line: ": [
            "Moreover, we achieve competitive single-model performance on CIFAR-100 with 16.53% error rate compared to state-of-the-art."
        ],
        "results: ": [
            [
                "error rate"
            ],
            [
                "16.53%"
            ]
        ]
    },
    "http://arxiv.org/abs/1808.05326v1": {
        "informative_line: ": [
            "Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "88%"
            ]
        ]
    },
    "https://arxiv.org/abs/1901.11117v4": {
        "informative_line: ": [
            "At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original \"big\" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters."
        ],
        "results: ": [
            [
                "BLEU score",
                "BLEU"
            ],
            [
                "29.8",
                "37.6%",
                "0.7"
            ]
        ]
    },
    "http://arxiv.org/abs/1809.08887v5": {
        "informative_line: ": [
            "We experiment with various state-of-the-art models and the best model achieves only 12.4% exact matching accuracy on a database split setting"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "12.4%"
            ]
        ]
    },
    "http://arxiv.org/abs/1511.07289v5": {
        "informative_line: ": [
            "On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "10%"
            ]
        ]
    },
    "http://arxiv.org/abs/1901.08043v3": {
        "informative_line: ": [
            "The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2% on COCO test-dev",
            "In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes",
            "Extreme point guided segmentation further improves this to 34.6% Mask AP."
        ],
        "results: ": [
            [
                "AP"
            ],
            [
                "43.2%"
            ]
        ]
    },
    "http://arxiv.org/abs/1503.03832v3": {
        "informative_line: ": [
            "On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%",
            "Our system cuts the error rate in comparison to the best published result by 30% on both datasets"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "99.63%"
            ]
        ]
    },
    "http://arxiv.org/abs/1512.03385v1": {
        "informative_line: ": [
            "An ensemble of these residual nets achieves 3.57% error on the ImageNet test set"
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "3.57%"
            ]
        ]
    },
    "http://arxiv.org/abs/1612.06851v2": {
        "informative_line: ": [
            "The proposed TDM architecture provides a significant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16, 35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any bells and whistles (e.g., multi-scale, iterative box refinement, etc.)."
        ],
        "results: ": [
            [
                "AP",
                "AP"
            ],
            [
                "28.6",
                "35.2",
                "37.3"
            ]
        ]
    },
    "http://arxiv.org/abs/1612.01925v1": {
        "informative_line: ": [
            "FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%"
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "2.0",
                "50%"
            ]
        ]
    },
    "http://arxiv.org/abs/1704.08803v2": {
        "informative_line: ": [
            "Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections"
        ],
        "results: ": [
            [
                "MAP"
            ],
            [
                "13%",
                "35%"
            ]
        ]
    },
    "http://arxiv.org/abs/1904.03797v1": {
        "informative_line: ": [
            "Without bells and whistles, FoveaBox achieves state-of-the-art single model performance of 42.1 AP on the standard COCO detection benchmark"
        ],
        "results: ": [
            [
                "AP"
            ],
            [
                "42.1"
            ]
        ]
    },
    "http://arxiv.org/abs/1611.08323v2": {
        "informative_line: ": [
            "Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset."
        ],
        "results: ": [
            [
                "score"
            ],
            [
                "71.8%"
            ]
        ]
    },
    "http://arxiv.org/abs/1705.08639v2": {
        "informative_line: ": [
            "In addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure"
        ],
        "results: ": [
            [
                "BPC",
                "BPC"
            ],
            [
                "1.20"
            ]
        ]
    },
    "http://arxiv.org/abs/1701.07717v5": {
        "informative_line: ": [
            "On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively"
        ],
        "results: ": [
            [
                "precision"
            ],
            [
                "4.37%",
                "1.6%",
                "2.46%"
            ]
        ]
    },
    "https://arxiv.org/abs/1404.3840v3": {
        "informative_line: ": [
            "Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark"
        ],
        "results: ": [
            [
                "accuracy",
                "accuracy"
            ],
            [
                "98.52%"
            ]
        ]
    },
    "http://arxiv.org/abs/1811.05181v1": {
        "informative_line: ": [
            "Without any whistles and bells, our model achieves 41.6 mAP on COCO test-dev set which surpasses the state-of-the-art method, Focal Loss (FL) + $SL_1$, by 0.8."
        ],
        "results: ": [
            [
                "mAP"
            ],
            [
                "41.6",
                "0.8"
            ]
        ]
    },
    "https://arxiv.org/abs/1811.06965v5": {
        "informative_line: ": [
            "We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models."
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "84.4%"
            ]
        ]
    },
    "http://arxiv.org/abs/1811.06965v4": {
        "informative_line: ": [
            "We train a 557 million parameters AmoebaNet model on ImageNet and achieve a new state-of-the-art 84.3% top-1 / 97.0% top-5 accuracy on ImageNet 2012 dataset",
            "Finally, we use this learned model to finetune multiple popular image classification datasets and obtain competitive results, including pushing the CIFAR-10 accuracy to 99% and CIFAR-100 accuracy to 91.3%."
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "84.3%",
                "97.0%"
            ]
        ]
    },
    "http://arxiv.org/abs/1609.05600v2": {
        "informative_line: ": [
            "The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e"
        ],
        "results: ": [
            [
                "accuracy",
                "accuracy"
            ],
            [
                "71.2%",
                "74.4%",
                "34.7%",
                "39.1%"
            ]
        ]
    },
    "http://arxiv.org/abs/1410.0736v4": {
        "informative_line: ": [
            "In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively."
        ],
        "results: ": [
            [
                "error"
            ],
            [
                "2.65%",
                "3.1%",
                "1.1%"
            ]
        ]
    },
    "http://arxiv.org/abs/1505.05899v1": {
        "informative_line: ": [
            "These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result."
        ],
        "results: ": [
            [
                "word error rate"
            ],
            [
                "8.0%",
                "23%"
            ]
        ]
    },
    "http://arxiv.org/abs/1604.08242v2": {
        "informative_line: ": [
            "We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset"
        ],
        "results: ": [
            [
                "word error rate"
            ],
            [
                "6.6%"
            ]
        ]
    },
    "https://arxiv.org/abs/1807.06653": {
        "informative_line: ": [
            "These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively",
            "The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised)"
        ],
        "results: ": [
            [
                "accuracy"
            ],
            [
                "6.6",
                "9.5"
            ]
        ]
    }
}