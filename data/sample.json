{
    "https://arxiv.org/abs/1906.02448": [
        "informative table: http://jiangsiyuan.com/table/0",
        {
            "Table 2: Factor analysis on Zh→En translation, the results are average BLEU scores on MT03∼06 datasets.": [
                [
                    "Best-Systems: RNNsearch+ sentence oracle+ noise",
                    "Best-Average: 40.09"
                ]
            ],
            "Table 3: Case-sensitive BLEU scores (%) on En→De task. The “‡” indicates the results are significantly better (p<0.01) than RNNsearch and Transformer.": [
                [
                    "Best-Systems: RNNsearch+ OR-NMT",
                    "Best-newstest2014: 27.41‡"
                ],
                [
                    "Best-Systems: Transformer (base)+ OR-NMT",
                    "Best-newstest2014: 28.65‡"
                ]
            ]
        }
    ],
    "http://arxiv.org/abs/1612.08083v3": [
        "informative table: http://jiangsiyuan.com/table/1"
    ],
    "http://arxiv.org/abs/1508.05326v1": [
        "informative table: http://jiangsiyuan.com/table/9",
        {
            "Table 4:   2-class test accuracy for two simple baseline systems included in the Excitement Open Platform, as well as SICK and RTE results for a model making use of more sophisticated lexical resources.": [
                [
                    "Best-System: Classifier Based+ Lexical Resources",
                    "Best-SNLI: 75.0"
                ],
                [
                    "Best-System: Classifier Based+ Lexical Resources",
                    "Best-SICK: 78.8"
                ],
                [
                    "Best-System: Classifier Based+ Lexical Resources",
                    "Best-RTE-3: 63.6"
                ]
            ]
        }
    ],
    "http://arxiv.org/abs/1802.05365v2": [
        "informative table: http://jiangsiyuan.com/table/6",
        {
            "Table 3: Development set performance for SQuAD, SNLI and SRL when including ELMo at different locations in the supervised model.": [
                [
                    "Best-Task: SQuAD",
                    "Best-Input & Output: 85.6"
                ],
                [
                    "Best-Task: SNLI",
                    "Best-Input & Output: 89.5"
                ],
                [
                    "Best-Task: SRL",
                    "Best-Input Only: 84.7"
                ]
            ],
            "Table 5: All-words fine grained WSD F1. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.": [
                [
                    "Best-Model: Iacobacci et al. (2016)",
                    "Best-F1: 70.1"
                ]
            ],
            "Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.": [
                [
                    "Best-Model: Ling et al. (2015)",
                    "Best-Acc.: 97.8"
                ]
            ],
            "Table 8: SNLI test set accuracy.444A comprehensive comparison can be found at https://nlp.stanford.edu/projects/snli/4Single model results occupy the portion, with ensemble results at the bottom.": [
                [
                    "Best-Model: ESIM+ELMo",
                    "Best-Acc.: 88.7 ± 0.17"
                ],
                [
                    "Best-Model: ESIM+ELMo ensemble",
                    "Best-Acc.: 89.3"
                ]
            ],
            "Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F1. The top half of the table contains single model results with ensembles at the bottom. References provided where available.": [
                [
                    "Best-Model: BiDAF + Self Attention + ELMo",
                    "Best-EM: 78.6"
                ],
                [
                    "Best-Model: BiDAF + Self Attention + ELMo",
                    "Best-F1: 85.8"
                ],
                [
                    "Best-Model: BiDAF + Self Attention + ELMo Ensemble",
                    "Best-EM: 81.0"
                ],
                [
                    "Best-Model: BiDAF + Self Attention + ELMo Ensemble",
                    "Best-F1: 87.4"
                ]
            ]
        }
    ]
}